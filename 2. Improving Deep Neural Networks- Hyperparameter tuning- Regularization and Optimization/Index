Objectives:
  + Recall that different types of initializations lead to different results
  + Recognize the importance of initialization in complex neural networks.
  + Recognize the difference between train/dev/test sets
  + Diagnose the bias and variance issues in your model
  + Learn when and how to use regularization methods such as dropout or L2 regularization.
  + Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them
  + Use gradient checking to verify the correctness of your backpropagation implementation
  + Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
  + Use random minibatches to accelerate the convergence and improve the optimization
  + Know the benefits of learning rate decay and apply it to your optimization
  + Master the process of hyperparameter tuning
  
Week 1:
 - Initialization.ipynb
 - Regularization.ipynb
 - Gradient Checking v1.ipynb
 
 Week 2:
  - Optimization methods.ipynb
 
 Week 3:
  - Tensorflow Tutorial.ipynb
